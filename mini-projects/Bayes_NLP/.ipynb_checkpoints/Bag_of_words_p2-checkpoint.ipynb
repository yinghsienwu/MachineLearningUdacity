{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Vectors\n",
    "==========\n",
    "Ref: https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors\n",
    "\n",
    "Goal: Using distributed word vectors created by the Word2Vec algorithm. Word2Vec (Google, 2013) is a neural network implementation that learns distributed representations for words. It does not need labels. \n",
    "\n",
    "Using word2vec in Python\n",
    "--------------\n",
    "We will use **gensim** package. We also need to install **cython** otherwise it will take days to run instead of minutes.\n",
    "\n",
    "Preparing to Train a Model\n",
    "------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data from files\n",
    "train=pd.read_csv(\"labeledTrainData.tsv\", header=0, delimiter=\"\\t\",quoting=3)\n",
    "test=pd.read_csv(\"testData.tsv\", header=0, delimiter=\"\\t\",quoting=3)\n",
    "unlabeled_train=pd.read_csv(\"unlabeledTrainData.tsv\",header=0,delimiter=\"\\t\",quoting=3)\n",
    "\n",
    "# Verify the number of reviews that were read (100,000 in total)\n",
    "print \"Read %d labeled train reviews, %d labeled test reviews, and %d unlabeled reviews\\n\" \\\n",
    "%(train[\"review\"].size, test[\"review\"].size, unlabeled_train[\"review\"].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train Word2Vec it is better not to remove stop words because the algorithm relies on the broader context of the sentence in order to produce high-quality word vectors. It is also better not to remove numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import various modules for string cleaning \n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_to_wordlist(review,remove_stopwords=False):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words. Returns a list of words.\n",
    "    # 1. Remove HTML\n",
    "    review_text=BeautifulSoup(review,\"html\").get_text()\n",
    "    # 2. Remove non-letters, keep numbers\n",
    "    review_text=re.sub(\"[^a-zA-Z0-9]\",\" \",review_text)\n",
    "    # 3. Convert words in lower case and split them\n",
    "    words=review_text.lower().split()\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops=set(stopwords.words(\"english\"))\n",
    "        words=[w for w in words if not w in stops]\n",
    "    # 5. return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec expects single sentences, each one as a list of words. It is not at all straightforward how to split a paragraph into sentences. There are all kinds of gotchas in natural language. English sentences can end with \"?\", \"!\", \"\"\", or \".\", among other things, and spacing and capitalization are not reliable guides either. For this reason, we'll use NLTK's **punkt** tokenizer for sentence splitting. In order to use this, you will need to install NLTK and use nltk.download() to download the relevant training file for punkt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download the punkt tokenizer for sentence splitting\n",
    "import nltk.data\n",
    "#nltk.download()  #I've done this before\n",
    "\n",
    "# Load the punkt tokenizer\n",
    "tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences(review,tokenizer,remove_stopwords=False):\n",
    "    # Function to split a review into parsed sentences. \n",
    "    # Returns a list of sentences, where each sentence is a list of words.\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences=tokenizer.tokenize(review.strip())\n",
    "    # 2. Loop over each sentence\n",
    "    sentences=[]\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence)>0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append(review_to_wordlist(raw_sentence,remove_stopwords))\n",
    "        # Return the list of sentences (each sentence is a list of words, so this returns a list of lists)\n",
    "    return sentences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n",
      "Parsing sentences from unlabeled set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anw4/anaconda/lib/python2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/Users/anw4/anaconda/lib/python2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/Users/anw4/anaconda/lib/python2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/Users/anw4/anaconda/lib/python2.7/site-packages/bs4/__init__.py:198: UserWarning: \"..\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n",
      "/Users/anw4/anaconda/lib/python2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/Users/anw4/anaconda/lib/python2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "print \"Parsing sentences from training set\"\n",
    "for review in train[\"review\"]:\n",
    "    review=review.decode(\"utf8\")\n",
    "    sentences+=review_to_sentences(review, tokenizer)\n",
    "    \n",
    "print \"Parsing sentences from unlabeled set\"\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    review=review.decode(\"utf8\")\n",
    "    sentences+=review_to_sentences(review, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795538\n",
      "[u'with', u'all', u'this', u'stuff', u'going', u'down', u'at', u'the', u'moment', u'with', u'mj', u'i', u've', u'started', u'listening', u'to', u'his', u'music', u'watching', u'the', u'odd', u'documentary', u'here', u'and', u'there', u'watched', u'the', u'wiz', u'and', u'watched', u'moonwalker', u'again']\n",
      "[u'maybe', u'i', u'just', u'want', u'to', u'get', u'a', u'certain', u'insight', u'into', u'this', u'guy', u'who', u'i', u'thought', u'was', u'really', u'cool', u'in', u'the', u'eighties', u'just', u'to', u'maybe', u'make', u'up', u'my', u'mind', u'whether', u'he', u'is', u'guilty', u'or', u'innocent']\n"
     ]
    }
   ],
   "source": [
    "# Check how many sentences we have in total- should be around 850,000+\n",
    "print len(sentences)\n",
    "print sentences[0]\n",
    "print sentences[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A minor detail to note is the difference between the \"+=\" and \"append\" when it comes to Python lists. In many applications the two are interchangeable, but here they are not. If you are appending a list of lists to another list of lists, \"append\" will only append the first list; you need to use \"+=\" in order to join all of the lists at once.\n",
    "Training and Saving Your Model\n",
    "-------------------\n",
    "- **Architecture**: skip-gram (default, slightly slower but produced better results) or continuous bag of words. \n",
    "- **Training algorithm**: Hierarchical softmax (default) or negative sampling.\n",
    "- **Downsampling of frequent words**: ranges from .00001 to .001. We use 0.001, which seems to improve the accuracy of the final model.\n",
    "- **Word vector dimensionality**: ranges from tens to thousands. We use 300. More features result in longer runtimes and often result in better models.\n",
    "- **Context / window size**: 10 seems to work well for hierarchical softmax. \n",
    "- **Minimum word count**: ranges from 10 to 100. We set to 40 to avoid attaching too much importance to individual movie titles. This results in an overall vocabulary size of ~15,000 words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s: %(levelname)s : %(message)s',level=logging.INFO)\n",
    "\n",
    "# Set values efor various parameters\n",
    "num_features = 300 # word vector dimensionality\n",
    "min_word_count = 40 \n",
    "num_workers= 4 # number of threads to run in parallel\n",
    "context= 10  # context window size\n",
    "downsampling= 1e-3 # downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (take some time)\n",
    "from gensim.models import word2vec\n",
    "print \"Training model...\"\n",
    "model=word2vec.Word2Vec(sentences, workers=num_workers, size=num_features, min_count=min_word_count,\\\n",
    "                       window=context, sample=downsampling)\n",
    "#If you don't plan to train the model any further, calling init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and save the model for later use.\n",
    "# You can load it later using Word2Vec.load()\n",
    "model_name=\"300features_40minwords_10context\"\n",
    "model.save(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the Model Results\n",
    "------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function \"doesnt_match\" can deduce which word in a set is most dissimilar from the others:\n",
    "model.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'berlin'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'woman', 0.6284092664718628),\n",
       " (u'lady', 0.5979578495025635),\n",
       " (u'lad', 0.5781458020210266),\n",
       " (u'men', 0.5217606425285339),\n",
       " (u'guy', 0.5173251032829285),\n",
       " (u'soldier', 0.5171582102775574),\n",
       " (u'monk', 0.5156159400939941),\n",
       " (u'businessman', 0.5037267208099365),\n",
       " (u'farmer', 0.5015547871589661),\n",
       " (u'millionaire', 0.4996732473373413)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function \"most_similar\" can get insight into the model's word clusters:\n",
    "model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'princess', 0.647876501083374),\n",
       " (u'latifah', 0.6203631162643433),\n",
       " (u'victoria', 0.6088835000991821),\n",
       " (u'maid', 0.607053279876709),\n",
       " (u'bride', 0.6010995507240295),\n",
       " (u'belle', 0.6008758544921875),\n",
       " (u'stepmother', 0.593067467212677),\n",
       " (u'goddess', 0.5916730165481567),\n",
       " (u'prince', 0.579993486404419),\n",
       " (u'dame', 0.5708566904067993)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'terrible', 0.762367844581604),\n",
       " (u'horrible', 0.7417336106300354),\n",
       " (u'atrocious', 0.7307676076889038),\n",
       " (u'dreadful', 0.7018641233444214),\n",
       " (u'abysmal', 0.6951020359992981),\n",
       " (u'horrendous', 0.6836704015731812),\n",
       " (u'horrid', 0.6676329374313354),\n",
       " (u'appalling', 0.6660313606262207),\n",
       " (u'lousy', 0.6142610311508179),\n",
       " (u'amateurish', 0.6114339828491211)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"awful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'wonderful', 0.7354174256324768),\n",
       " (u'terrific', 0.7346562147140503),\n",
       " (u'fantastic', 0.7278304696083069),\n",
       " (u'superb', 0.6601248979568481),\n",
       " (u'fine', 0.6429445743560791),\n",
       " (u'excellent', 0.6395628452301025),\n",
       " (u'brilliant', 0.6257189512252808),\n",
       " (u'good', 0.6159985065460205),\n",
       " (u'marvelous', 0.6155490279197693),\n",
       " (u'fabulous', 0.5945401191711426)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"great\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a reasonably good model for semantic meaning- at least as good as Bag of Words. Next, we are going to use these fancy distributed word vectors for supervised learning.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
