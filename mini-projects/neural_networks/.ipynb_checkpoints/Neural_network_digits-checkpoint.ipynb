{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network\n",
    "=============\n",
    "Ref: https://rolisz.ro/2013/04/18/neural-networks-in-python/\n",
    "\n",
    "Define activation functions and their derivatives:\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "def tanh_deriv(x):\n",
    "    return 1.0-np.tanh(x)\n",
    "def logistic(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def logistic_derivative(x):\n",
    "    return logistic(x)*(1-logistic(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to set the number of neurons in each layer, initialize their weights randomly between -0.25 and 0.25 and set the activation function to be used. Each layer, except the last one, will also have a bias unit which cor­re­sponds to the threshold value for the activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, activation='tanh'):\n",
    "        \"\"\"\n",
    "        :param layers: A list containing the number of units in each layer.\n",
    "        Should be at least two values\n",
    "        :param activation: The activation function to be used. Can be\n",
    "        \"logistic\" or \"tanh\"\n",
    "        \"\"\"\n",
    "        if activation == 'logistic':\n",
    "            self.activation = logistic\n",
    "            self.activation_deriv = logistic_derivative\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = tanh\n",
    "            self.activation_deriv = tanh_deriv\n",
    "\n",
    "        self.weights = []\n",
    "        for i in range(1, len(layers) - 1):\n",
    "            self.weights.append((2*np.random.random((layers[i - 1] + 1, layers[i]+ 1))-1)*0.25)\n",
    "        self.weights.append((2*np.random.random((layers[i] + 1, layers[i +1]))-1)*0.25)\n",
    "\n",
    "    ##Training##\n",
    "    '''\n",
    "    Given a set of input vectors X and output values y, adjust the weights appropriately. The algorithm we will use \n",
    "    is called **stochastic gradient descent**, which chooses randomly a sample from the training data and does the \n",
    "    backpropagation for that sample, and this is repeated for a number of times (called epochs). We also have to set \n",
    "    the learning rate of the algorithm, which determines how big a change occurs in the weights each time \n",
    "    (proportionally to the errors).\n",
    "    '''\n",
    "\n",
    "    def fit(self, X, y, learning_rate=0.2, epochs=10000):\n",
    "        X = np.atleast_2d(X)\n",
    "        temp = np.ones([X.shape[0], X.shape[1]+1])\n",
    "        temp[:, 0:-1] = X  # adding the bias unit to the input layer\n",
    "        X = temp\n",
    "        y = np.array(y)\n",
    "\n",
    "        for k in range(epochs):\n",
    "            i = np.random.randint(X.shape[0])\n",
    "            a = [X[i]]\n",
    "\n",
    "            for l in range(len(self.weights)):\n",
    "                a.append(self.activation(np.dot(a[l], self.weights[l])))\n",
    "            error = y[i] - a[-1]\n",
    "            deltas = [error * self.activation_deriv(a[-1])]\n",
    "\n",
    "            for l in range(len(a) - 2, 0, -1): # we need to begin at the second to last layer\n",
    "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_deriv(a[l]))\n",
    "            deltas.reverse()\n",
    "            for i in range(len(self.weights)):\n",
    "                layer = np.atleast_2d(a[i])\n",
    "                delta = np.atleast_2d(deltas[i])\n",
    "                self.weights[i] += learning_rate * layer.T.dot(delta)\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = np.array(x)\n",
    "        temp = np.ones(x.shape[0]+1)\n",
    "        temp[0:-1] = x\n",
    "        a = temp\n",
    "        for l in range(0, len(self.weights)):\n",
    "            a = self.activation(np.dot(a, self.weights[l]))\n",
    "        return a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing-XOR\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 0]), array([ 0.06458202]))\n",
      "(array([0, 1]), array([ 0.82248188]))\n",
      "(array([1, 0]), array([ 0.81177321]))\n",
      "(array([1, 1]), array([-0.31999041]))\n"
     ]
    }
   ],
   "source": [
    "nn=NeuralNetwork([2,2,1],'tanh')\n",
    "X=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y=np.array([0,1,1,0])\n",
    "nn.fit(X,y)\n",
    "for i in X:\n",
    "    print (i,nn.predict(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing- digits dataset\n",
    "-------------------\n",
    "This has 1797 8x8 pixel images of digits with their labels. Lets see what accuracies can we get on them. We will have to transform the labels from values (such as 1 or 5), to vectors of 10 elements, which are all 0 except for the position corresponding to the label, which will be one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAEACAYAAACatzzfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAMn9FVd1IsusNmvXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115831a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split \n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "digits = load_digits()\n",
    "print(digits.data.shape)\n",
    "plt.pcolor(digits.images[0]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 8, 9, 8])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = digits.data\n",
    "y = digits.target\n",
    "X -= X.min() # normalize the values to bring them into the range 0-1\n",
    "X /= X.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[51  0  0  0  0  1  0  0  0  0]\n",
      " [ 0 44  0  0  0  2  0  0  0  1]\n",
      " [ 0  1 51  1  0  0  0  0  0  0]\n",
      " [ 0  1  0 42  0  1  0  1  0  0]\n",
      " [ 0  0  0  0 34  0  0  0  0  1]\n",
      " [ 0  0  0  0  0 41  0  0  0  3]\n",
      " [ 0  0  0  0  0  0 43  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 41  0  0]\n",
      " [ 0  1  0  0  0  0  0  0 34  4]\n",
      " [ 0  0  0  0  0  1  0  0  0 50]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.98      0.99        52\n",
      "          1       0.94      0.94      0.94        47\n",
      "          2       1.00      0.96      0.98        53\n",
      "          3       0.98      0.93      0.95        45\n",
      "          4       1.00      0.97      0.99        35\n",
      "          5       0.89      0.93      0.91        44\n",
      "          6       1.00      1.00      1.00        43\n",
      "          7       0.98      1.00      0.99        41\n",
      "          8       1.00      0.87      0.93        39\n",
      "          9       0.85      0.98      0.91        51\n",
      "\n",
      "avg / total       0.96      0.96      0.96       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork([64,100,10],'logistic')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "labels_train = LabelBinarizer().fit_transform(y_train)\n",
    "labels_test = LabelBinarizer().fit_transform(y_test)\n",
    "\n",
    "nn.fit(X_train,labels_train,epochs=30000)\n",
    "predictions = []\n",
    "for i in range(X_test.shape[0]):\n",
    "    o = nn.predict(X_test[i] )\n",
    "    predictions.append(np.argmax(o))\n",
    "print confusion_matrix(y_test,predictions)\n",
    "print classification_report(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
